{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6148b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71fbee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1530fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f52fc3db10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa69ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, Open: string, High: string, Low: string, Close: string, Volume: string, Dividends: string, Stock Splits: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read dataset\n",
    "\n",
    "spark.read.option('header','true').csv('TSLA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7344c30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+---------+------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|Dividends|Stock Splits|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+---------+------------+\n",
      "|2019-05-21|39.551998138427734| 41.47999954223633| 39.20800018310547| 41.01599884033203| 90019500|        0|         0.0|\n",
      "|2019-05-22| 39.81999969482422| 40.78799819946289| 38.35599899291992| 38.54600143432617| 93426000|        0|         0.0|\n",
      "|2019-05-23| 38.86800003051758| 39.89400100708008| 37.24399948120117|39.097999572753906|132735500|        0|         0.0|\n",
      "|2019-05-24|39.965999603271484| 39.99599838256836|             37.75|38.125999450683594| 70683000|        0|         0.0|\n",
      "|2019-05-28|  38.2400016784668|              39.0| 37.56999969482422|  37.7400016784668| 51564500|        0|         0.0|\n",
      "|2019-05-29| 37.41999816894531| 38.47800064086914|37.007999420166016| 37.97200012207031| 59843000|        0|         0.0|\n",
      "|2019-05-30|             37.75| 38.45199966430664| 37.40399932861328| 37.64400100708008| 39632500|        0|         0.0|\n",
      "|2019-05-31| 37.02000045776367| 37.98400115966797| 36.81999969482422| 37.03200149536133| 52033500|        0|         0.0|\n",
      "|2019-06-03| 37.10200119018555| 37.33599853515625| 35.39799880981445| 35.79399871826172| 65322000|        0|         0.0|\n",
      "|2019-06-04|36.220001220703125| 38.79600143432617|35.922000885009766|38.720001220703125| 69037500|        0|         0.0|\n",
      "|2019-06-05|39.736000061035156| 40.25600051879883|38.369998931884766| 39.31800079345703| 67554000|        0|         0.0|\n",
      "|2019-06-06| 40.88800048828125| 42.20000076293945| 40.36000061035156|41.189998626708984|101211000|        0|         0.0|\n",
      "|2019-06-07|              41.0|42.167999267578125| 40.70000076293945|40.900001525878906| 80017500|        0|         0.0|\n",
      "|2019-06-10| 42.04999923706055| 43.38800048828125|41.801998138427734| 42.57600021362305| 52925000|        0|         0.0|\n",
      "|2019-06-11|43.827999114990234| 44.18000030517578| 42.70000076293945| 43.41999816894531| 58267500|        0|         0.0|\n",
      "|2019-06-12| 44.59000015258789| 44.67599868774414| 41.79999923706055| 41.85200119018555| 75987500|        0|         0.0|\n",
      "|2019-06-13| 42.07600021362305| 42.97999954223633| 41.50199890136719| 42.78200149536133| 40841500|        0|         0.0|\n",
      "|2019-06-14|             42.25| 43.33000183105469| 42.08000183105469| 42.98400115966797| 37167000|        0|         0.0|\n",
      "|2019-06-17| 43.09600067138672|45.400001525878906|42.854000091552734| 45.00600051879883| 61584000|        0|         0.0|\n",
      "|2019-06-18| 45.74399948120117|46.948001861572266|44.512001037597656|44.948001861572266| 63579000|        0|         0.0|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## See dataset\n",
    "spark.read.option('header','true').csv('TSLA.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01db89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv('TSLA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20dfa339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Dividends: string (nullable = true)\n",
      " |-- Stock Splits: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check Schema\n",
    "df_pyspark.printSchema()   # Here it is giving every fields as string even if some fields should be interger or other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8414a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to make all fields as per thier data type\n",
    "\n",
    "df_pyspark=spark.read.option('header','true').csv('TSLA.csv', inferSchema=True) # Adding inferSchema=True for fields data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "956f6091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Dividends: integer (nullable = true)\n",
      " |-- Stock Splits: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47429c22",
   "metadata": {},
   "source": [
    "### Another way of doing the same data read and schema check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7a2f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('TSLA.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31a5b5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+---------+------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|Dividends|Stock Splits|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+---------+------------+\n",
      "|2019-05-21|39.551998138427734| 41.47999954223633| 39.20800018310547| 41.01599884033203| 90019500|        0|         0.0|\n",
      "|2019-05-22| 39.81999969482422| 40.78799819946289| 38.35599899291992| 38.54600143432617| 93426000|        0|         0.0|\n",
      "|2019-05-23| 38.86800003051758| 39.89400100708008| 37.24399948120117|39.097999572753906|132735500|        0|         0.0|\n",
      "|2019-05-24|39.965999603271484| 39.99599838256836|             37.75|38.125999450683594| 70683000|        0|         0.0|\n",
      "|2019-05-28|  38.2400016784668|              39.0| 37.56999969482422|  37.7400016784668| 51564500|        0|         0.0|\n",
      "|2019-05-29| 37.41999816894531| 38.47800064086914|37.007999420166016| 37.97200012207031| 59843000|        0|         0.0|\n",
      "|2019-05-30|             37.75| 38.45199966430664| 37.40399932861328| 37.64400100708008| 39632500|        0|         0.0|\n",
      "|2019-05-31| 37.02000045776367| 37.98400115966797| 36.81999969482422| 37.03200149536133| 52033500|        0|         0.0|\n",
      "|2019-06-03| 37.10200119018555| 37.33599853515625| 35.39799880981445| 35.79399871826172| 65322000|        0|         0.0|\n",
      "|2019-06-04|36.220001220703125| 38.79600143432617|35.922000885009766|38.720001220703125| 69037500|        0|         0.0|\n",
      "|2019-06-05|39.736000061035156| 40.25600051879883|38.369998931884766| 39.31800079345703| 67554000|        0|         0.0|\n",
      "|2019-06-06| 40.88800048828125| 42.20000076293945| 40.36000061035156|41.189998626708984|101211000|        0|         0.0|\n",
      "|2019-06-07|              41.0|42.167999267578125| 40.70000076293945|40.900001525878906| 80017500|        0|         0.0|\n",
      "|2019-06-10| 42.04999923706055| 43.38800048828125|41.801998138427734| 42.57600021362305| 52925000|        0|         0.0|\n",
      "|2019-06-11|43.827999114990234| 44.18000030517578| 42.70000076293945| 43.41999816894531| 58267500|        0|         0.0|\n",
      "|2019-06-12| 44.59000015258789| 44.67599868774414| 41.79999923706055| 41.85200119018555| 75987500|        0|         0.0|\n",
      "|2019-06-13| 42.07600021362305| 42.97999954223633| 41.50199890136719| 42.78200149536133| 40841500|        0|         0.0|\n",
      "|2019-06-14|             42.25| 43.33000183105469| 42.08000183105469| 42.98400115966797| 37167000|        0|         0.0|\n",
      "|2019-06-17| 43.09600067138672|45.400001525878906|42.854000091552734| 45.00600051879883| 61584000|        0|         0.0|\n",
      "|2019-06-18| 45.74399948120117|46.948001861572266|44.512001037597656|44.948001861572266| 63579000|        0|         0.0|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23f6fae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Dividends: integer (nullable = true)\n",
      " |-- Stock Splits: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e992994c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get all column names\n",
    "\n",
    "df_pyspark.columns  # Same way as pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc8a20d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2019, 5, 21), Open=39.551998138427734, High=41.47999954223633, Low=39.20800018310547, Close=41.01599884033203, Volume=90019500, Dividends=0, Stock Splits=0.0),\n",
       " Row(Date=datetime.date(2019, 5, 22), Open=39.81999969482422, High=40.78799819946289, Low=38.35599899291992, Close=38.54600143432617, Volume=93426000, Dividends=0, Stock Splits=0.0),\n",
       " Row(Date=datetime.date(2019, 5, 23), Open=38.86800003051758, High=39.89400100708008, Low=37.24399948120117, Close=39.097999572753906, Volume=132735500, Dividends=0, Stock Splits=0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca0172",
   "metadata": {},
   "source": [
    "### To see any one column data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87c9762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------+-----------------+------------------+---------+---------+------------+\n",
      "|      Date|              Open|             High|              Low|             Close|   Volume|Dividends|Stock Splits|\n",
      "+----------+------------------+-----------------+-----------------+------------------+---------+---------+------------+\n",
      "|2019-05-21|39.551998138427734|41.47999954223633|39.20800018310547| 41.01599884033203| 90019500|        0|         0.0|\n",
      "|2019-05-22| 39.81999969482422|40.78799819946289|38.35599899291992| 38.54600143432617| 93426000|        0|         0.0|\n",
      "|2019-05-23| 38.86800003051758|39.89400100708008|37.24399948120117|39.097999572753906|132735500|        0|         0.0|\n",
      "|2019-05-24|39.965999603271484|39.99599838256836|            37.75|38.125999450683594| 70683000|        0|         0.0|\n",
      "|2019-05-28|  38.2400016784668|             39.0|37.56999969482422|  37.7400016784668| 51564500|        0|         0.0|\n",
      "+----------+------------------+-----------------+-----------------+------------------+---------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f31d493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Open'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will not work like pandas\n",
    "\n",
    "df_pyspark['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "540454fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_pyspark[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "df_pyspark['Open'].show()  # It will not work in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bde1566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Open: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select('Open')  # It show that retrun is dataframe of pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b889d9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              Open|\n",
      "+------------------+\n",
      "|39.551998138427734|\n",
      "| 39.81999969482422|\n",
      "| 38.86800003051758|\n",
      "|39.965999603271484|\n",
      "|  38.2400016784668|\n",
      "| 37.41999816894531|\n",
      "+------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Open').show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5111a168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   Volume|\n",
      "+---------+\n",
      "| 90019500|\n",
      "| 93426000|\n",
      "|132735500|\n",
      "| 70683000|\n",
      "| 51564500|\n",
      "| 59843000|\n",
      "+---------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Volume').show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "233944a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   Volume|\n",
      "+---------+\n",
      "| 90019500|\n",
      "| 93426000|\n",
      "|132735500|\n",
      "| 70683000|\n",
      "| 51564500|\n",
      "| 59843000|\n",
      "+---------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Volume']).show(6)  ## We can pass in list form or without list for single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1203eec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|              Open|   Volume|\n",
      "+------------------+---------+\n",
      "|39.551998138427734| 90019500|\n",
      "| 39.81999969482422| 93426000|\n",
      "| 38.86800003051758|132735500|\n",
      "|39.965999603271484| 70683000|\n",
      "|  38.2400016784668| 51564500|\n",
      "| 37.41999816894531| 59843000|\n",
      "|             37.75| 39632500|\n",
      "| 37.02000045776367| 52033500|\n",
      "| 37.10200119018555| 65322000|\n",
      "|36.220001220703125| 69037500|\n",
      "|39.736000061035156| 67554000|\n",
      "| 40.88800048828125|101211000|\n",
      "|              41.0| 80017500|\n",
      "| 42.04999923706055| 52925000|\n",
      "|43.827999114990234| 58267500|\n",
      "| 44.59000015258789| 75987500|\n",
      "| 42.07600021362305| 40841500|\n",
      "|             42.25| 37167000|\n",
      "| 43.09600067138672| 61584000|\n",
      "| 45.74399948120117| 63579000|\n",
      "+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## for Multiple columns pass them in list form\n",
    "\n",
    "df_pyspark.select(['Open','Volume']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba20d2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'date'),\n",
       " ('Open', 'double'),\n",
       " ('High', 'double'),\n",
       " ('Low', 'double'),\n",
       " ('Close', 'double'),\n",
       " ('Volume', 'int'),\n",
       " ('Dividends', 'int'),\n",
       " ('Stock Splits', 'double')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see only data types of the columns\n",
    "\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5123cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+------------------+-------------------+---------+--------------------+\n",
      "|summary|              Open|             High|              Low|             Close|             Volume|Dividends|        Stock Splits|\n",
      "+-------+------------------+-----------------+-----------------+------------------+-------------------+---------+--------------------+\n",
      "|  count|               758|              758|              758|               758|                758|      758|                 758|\n",
      "|   mean|485.87698444275867|497.1353471952252|473.3162246845014|485.53151336005624|4.693252387862797E7|      0.0|0.006596306068601583|\n",
      "| stddev|353.89717333664964|361.7621027606688|344.5818545767559|353.16035279439683| 3.39384323263938E7|      0.0| 0.18160817807303695|\n",
      "|    min|36.220001220703125|37.33599853515625|35.39799880981445| 35.79399871826172|            9800600|        0|                 0.0|\n",
      "|    max|1234.4100341796875|1243.489990234375|           1217.0|1229.9100341796875|          304694000|        0|                 5.0|\n",
      "+-------+------------------+-----------------+-----------------+------------------+-------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to see describe open pyspark\n",
    "\n",
    "df_pyspark.describe().show()  # It will show summary of the dataset similar to pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb9861",
   "metadata": {},
   "source": [
    "### Adding column in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03c92239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: date, Open: double, High: double, Low: double, Close: double, Volume: int, Dividends: int, Stock Splits: double, Date + 2: date]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.withColumn('Date + 2', df_pyspark['Date']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5af2f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------+-----------------+------------------+---------+---------+------------+----------+\n",
      "|      Date|              Open|             High|              Low|             Close|   Volume|Dividends|Stock Splits|  Date + 2|\n",
      "+----------+------------------+-----------------+-----------------+------------------+---------+---------+------------+----------+\n",
      "|2019-05-21|39.551998138427734|41.47999954223633|39.20800018310547| 41.01599884033203| 90019500|        0|         0.0|2019-05-23|\n",
      "|2019-05-22| 39.81999969482422|40.78799819946289|38.35599899291992| 38.54600143432617| 93426000|        0|         0.0|2019-05-24|\n",
      "|2019-05-23| 38.86800003051758|39.89400100708008|37.24399948120117|39.097999572753906|132735500|        0|         0.0|2019-05-25|\n",
      "|2019-05-24|39.965999603271484|39.99599838256836|            37.75|38.125999450683594| 70683000|        0|         0.0|2019-05-26|\n",
      "|2019-05-28|  38.2400016784668|             39.0|37.56999969482422|  37.7400016784668| 51564500|        0|         0.0|2019-05-30|\n",
      "+----------+------------------+-----------------+-----------------+------------------+---------+---------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumn('Date + 2', df_pyspark['Date']+2).show(5) # New column added with date 2 days more at last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75564831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=df_pyspark.withColumn('Date + 2', df_pyspark['Date']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "856a4f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------+-----------------+------------------+---------+---------+------------+----------+\n",
      "|      Date|              Open|             High|              Low|             Close|   Volume|Dividends|Stock Splits|  Date + 2|\n",
      "+----------+------------------+-----------------+-----------------+------------------+---------+---------+------------+----------+\n",
      "|2019-05-21|39.551998138427734|41.47999954223633|39.20800018310547| 41.01599884033203| 90019500|        0|         0.0|2019-05-23|\n",
      "|2019-05-22| 39.81999969482422|40.78799819946289|38.35599899291992| 38.54600143432617| 93426000|        0|         0.0|2019-05-24|\n",
      "|2019-05-23| 38.86800003051758|39.89400100708008|37.24399948120117|39.097999572753906|132735500|        0|         0.0|2019-05-25|\n",
      "|2019-05-24|39.965999603271484|39.99599838256836|            37.75|38.125999450683594| 70683000|        0|         0.0|2019-05-26|\n",
      "+----------+------------------+-----------------+-----------------+------------------+---------+---------+------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938e72e",
   "metadata": {},
   "source": [
    "### To drop any columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8f8403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+---------+------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|Dividends|Stock Splits|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+---------+------------+\n",
      "|2019-05-21|39.551998138427734| 41.47999954223633| 39.20800018310547| 41.01599884033203| 90019500|        0|         0.0|\n",
      "|2019-05-22| 39.81999969482422| 40.78799819946289| 38.35599899291992| 38.54600143432617| 93426000|        0|         0.0|\n",
      "|2019-05-23| 38.86800003051758| 39.89400100708008| 37.24399948120117|39.097999572753906|132735500|        0|         0.0|\n",
      "|2019-05-24|39.965999603271484| 39.99599838256836|             37.75|38.125999450683594| 70683000|        0|         0.0|\n",
      "|2019-05-28|  38.2400016784668|              39.0| 37.56999969482422|  37.7400016784668| 51564500|        0|         0.0|\n",
      "|2019-05-29| 37.41999816894531| 38.47800064086914|37.007999420166016| 37.97200012207031| 59843000|        0|         0.0|\n",
      "|2019-05-30|             37.75| 38.45199966430664| 37.40399932861328| 37.64400100708008| 39632500|        0|         0.0|\n",
      "|2019-05-31| 37.02000045776367| 37.98400115966797| 36.81999969482422| 37.03200149536133| 52033500|        0|         0.0|\n",
      "|2019-06-03| 37.10200119018555| 37.33599853515625| 35.39799880981445| 35.79399871826172| 65322000|        0|         0.0|\n",
      "|2019-06-04|36.220001220703125| 38.79600143432617|35.922000885009766|38.720001220703125| 69037500|        0|         0.0|\n",
      "|2019-06-05|39.736000061035156| 40.25600051879883|38.369998931884766| 39.31800079345703| 67554000|        0|         0.0|\n",
      "|2019-06-06| 40.88800048828125| 42.20000076293945| 40.36000061035156|41.189998626708984|101211000|        0|         0.0|\n",
      "|2019-06-07|              41.0|42.167999267578125| 40.70000076293945|40.900001525878906| 80017500|        0|         0.0|\n",
      "|2019-06-10| 42.04999923706055| 43.38800048828125|41.801998138427734| 42.57600021362305| 52925000|        0|         0.0|\n",
      "|2019-06-11|43.827999114990234| 44.18000030517578| 42.70000076293945| 43.41999816894531| 58267500|        0|         0.0|\n",
      "|2019-06-12| 44.59000015258789| 44.67599868774414| 41.79999923706055| 41.85200119018555| 75987500|        0|         0.0|\n",
      "|2019-06-13| 42.07600021362305| 42.97999954223633| 41.50199890136719| 42.78200149536133| 40841500|        0|         0.0|\n",
      "|2019-06-14|             42.25| 43.33000183105469| 42.08000183105469| 42.98400115966797| 37167000|        0|         0.0|\n",
      "|2019-06-17| 43.09600067138672|45.400001525878906|42.854000091552734| 45.00600051879883| 61584000|        0|         0.0|\n",
      "|2019-06-18| 45.74399948120117|46.948001861572266|44.512001037597656|44.948001861572266| 63579000|        0|         0.0|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Date + 2').show() # Last column is dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76d91998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=df_pyspark.drop('Date + 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b26d30c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------+-----------------+------------------+---------+---------+------------+\n",
      "|      Date|              Open|             High|              Low|             Close|   Volume|Dividends|Stock Splits|\n",
      "+----------+------------------+-----------------+-----------------+------------------+---------+---------+------------+\n",
      "|2019-05-21|39.551998138427734|41.47999954223633|39.20800018310547| 41.01599884033203| 90019500|        0|         0.0|\n",
      "|2019-05-22| 39.81999969482422|40.78799819946289|38.35599899291992| 38.54600143432617| 93426000|        0|         0.0|\n",
      "|2019-05-23| 38.86800003051758|39.89400100708008|37.24399948120117|39.097999572753906|132735500|        0|         0.0|\n",
      "+----------+------------------+-----------------+-----------------+------------------+---------+---------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43039a",
   "metadata": {},
   "source": [
    "### Rename column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6eb31eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------------+------------------+------------------+---------+---------+------------+\n",
      "|Checking Date|              Open|              High|               Low|             Close|   Volume|Dividends|Stock Splits|\n",
      "+-------------+------------------+------------------+------------------+------------------+---------+---------+------------+\n",
      "|   2019-05-21|39.551998138427734| 41.47999954223633| 39.20800018310547| 41.01599884033203| 90019500|        0|         0.0|\n",
      "|   2019-05-22| 39.81999969482422| 40.78799819946289| 38.35599899291992| 38.54600143432617| 93426000|        0|         0.0|\n",
      "|   2019-05-23| 38.86800003051758| 39.89400100708008| 37.24399948120117|39.097999572753906|132735500|        0|         0.0|\n",
      "|   2019-05-24|39.965999603271484| 39.99599838256836|             37.75|38.125999450683594| 70683000|        0|         0.0|\n",
      "|   2019-05-28|  38.2400016784668|              39.0| 37.56999969482422|  37.7400016784668| 51564500|        0|         0.0|\n",
      "|   2019-05-29| 37.41999816894531| 38.47800064086914|37.007999420166016| 37.97200012207031| 59843000|        0|         0.0|\n",
      "|   2019-05-30|             37.75| 38.45199966430664| 37.40399932861328| 37.64400100708008| 39632500|        0|         0.0|\n",
      "|   2019-05-31| 37.02000045776367| 37.98400115966797| 36.81999969482422| 37.03200149536133| 52033500|        0|         0.0|\n",
      "|   2019-06-03| 37.10200119018555| 37.33599853515625| 35.39799880981445| 35.79399871826172| 65322000|        0|         0.0|\n",
      "|   2019-06-04|36.220001220703125| 38.79600143432617|35.922000885009766|38.720001220703125| 69037500|        0|         0.0|\n",
      "|   2019-06-05|39.736000061035156| 40.25600051879883|38.369998931884766| 39.31800079345703| 67554000|        0|         0.0|\n",
      "|   2019-06-06| 40.88800048828125| 42.20000076293945| 40.36000061035156|41.189998626708984|101211000|        0|         0.0|\n",
      "|   2019-06-07|              41.0|42.167999267578125| 40.70000076293945|40.900001525878906| 80017500|        0|         0.0|\n",
      "|   2019-06-10| 42.04999923706055| 43.38800048828125|41.801998138427734| 42.57600021362305| 52925000|        0|         0.0|\n",
      "|   2019-06-11|43.827999114990234| 44.18000030517578| 42.70000076293945| 43.41999816894531| 58267500|        0|         0.0|\n",
      "|   2019-06-12| 44.59000015258789| 44.67599868774414| 41.79999923706055| 41.85200119018555| 75987500|        0|         0.0|\n",
      "|   2019-06-13| 42.07600021362305| 42.97999954223633| 41.50199890136719| 42.78200149536133| 40841500|        0|         0.0|\n",
      "|   2019-06-14|             42.25| 43.33000183105469| 42.08000183105469| 42.98400115966797| 37167000|        0|         0.0|\n",
      "|   2019-06-17| 43.09600067138672|45.400001525878906|42.854000091552734| 45.00600051879883| 61584000|        0|         0.0|\n",
      "|   2019-06-18| 45.74399948120117|46.948001861572266|44.512001037597656|44.948001861572266| 63579000|        0|         0.0|\n",
      "+-------------+------------------+------------------+------------------+------------------+---------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('Date', 'Checking Date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bae75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
