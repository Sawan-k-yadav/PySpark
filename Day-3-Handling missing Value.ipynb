{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54af544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8aa4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc8706df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c2aea906d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c995295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Salary: int, Experience: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv('employee-details.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c09d9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+----------+\n",
      "|     Name| Age|Salary|Experience|\n",
      "+---------+----+------+----------+\n",
      "|     John|  25| 50000|         5|\n",
      "|     Emma|NULL| 60000|         3|\n",
      "|  Michael|  30|  NULL|         7|\n",
      "|   Sophia|  28| 55000|      NULL|\n",
      "|   Oliver|  35| 70000|        10|\n",
      "|      Ava|NULL| 45000|         2|\n",
      "|  William|  40|  NULL|        12|\n",
      "|Charlotte|  22| 40000|         1|\n",
      "|    David|  45| 80000|      NULL|\n",
      "+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('employee-details.csv', header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c5953a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('employee-details.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62d467a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----------+\n",
      "| Age|Salary|Experience|\n",
      "+----+------+----------+\n",
      "|  25| 50000|         5|\n",
      "|NULL| 60000|         3|\n",
      "|  30|  NULL|         7|\n",
      "|  28| 55000|      NULL|\n",
      "|  35| 70000|        10|\n",
      "|NULL| 45000|         2|\n",
      "|  40|  NULL|        12|\n",
      "|  22| 40000|         1|\n",
      "|  45| 80000|      NULL|\n",
      "+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For dropping any columns\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8f60efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+----------+\n",
      "|     Name| Age|Salary|Experience|\n",
      "+---------+----+------+----------+\n",
      "|     John|  25| 50000|         5|\n",
      "|     Emma|NULL| 60000|         3|\n",
      "|  Michael|  30|  NULL|         7|\n",
      "|   Sophia|  28| 55000|      NULL|\n",
      "|   Oliver|  35| 70000|        10|\n",
      "|      Ava|NULL| 45000|         2|\n",
      "|  William|  40|  NULL|        12|\n",
      "|Charlotte|  22| 40000|         1|\n",
      "|    David|  45| 80000|      NULL|\n",
      "+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a8f56a",
   "metadata": {},
   "source": [
    "### For dropping any missing/null row of any columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acee69b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+\n",
      "|     Name|Age|Salary|Experience|\n",
      "+---------+---+------+----------+\n",
      "|     John| 25| 50000|         5|\n",
      "|   Oliver| 35| 70000|        10|\n",
      "|Charlotte| 22| 40000|         1|\n",
      "+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show() # It will drop all the null values from every columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "613dcfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How ro drop row wise for specifc not all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "793d8f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+----------+\n",
      "|     Name| Age|Salary|Experience|\n",
      "+---------+----+------+----------+\n",
      "|     John|  25| 50000|         5|\n",
      "|     Emma|NULL| 60000|         3|\n",
      "|  Michael|  30|  NULL|         7|\n",
      "|   Sophia|  28| 55000|      NULL|\n",
      "|   Oliver|  35| 70000|        10|\n",
      "|      Ava|NULL| 45000|         2|\n",
      "|  William|  40|  NULL|        12|\n",
      "|Charlotte|  22| 40000|         1|\n",
      "|    David|  45| 80000|      NULL|\n",
      "+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## how=='any' (by default) and how=='all'\n",
    "\n",
    "\n",
    "df_pyspark.na.drop(how='all').show() # If any has null value for all the fields then it will drop all those null from all field\n",
    "\n",
    "## But here there is no any row which has null for all the fields/columns, hence it doesn't drop anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ca50239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+\n",
      "|     Name|Age|Salary|Experience|\n",
      "+---------+---+------+----------+\n",
      "|     John| 25| 50000|         5|\n",
      "|   Oliver| 35| 70000|        10|\n",
      "|Charlotte| 22| 40000|         1|\n",
      "+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any').show()  # It will drop all the null values from anywhere in any of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ebd447b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+----------+\n",
      "|     Name| Age|Salary|Experience|\n",
      "+---------+----+------+----------+\n",
      "|     John|  25| 50000|         5|\n",
      "|     Emma|NULL| 60000|         3|\n",
      "|  Michael|  30|  NULL|         7|\n",
      "|   Sophia|  28| 55000|      NULL|\n",
      "|   Oliver|  35| 70000|        10|\n",
      "|      Ava|NULL| 45000|         2|\n",
      "|  William|  40|  NULL|        12|\n",
      "|Charlotte|  22| 40000|         1|\n",
      "|    David|  45| 80000|      NULL|\n",
      "+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## threshold\n",
    "\n",
    "df_pyspark.na.drop(how='any', thresh=2).show()\n",
    "\n",
    "## It checks, in each row if there is less than 2 values which are not null.\n",
    "\n",
    "## If not null are equal or more than threshold value then it will not drop\n",
    "## but if not null value are less than thresh value then it will drop that whole row.\n",
    "\n",
    "\n",
    "# Here all the row having more than threshold value. hence non of them are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af67759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+----------+\n",
      "|     Name| Age|Salary|Experience|\n",
      "+---------+----+------+----------+\n",
      "|     John|  25| 50000|         5|\n",
      "|     Emma|NULL| 60000|         3|\n",
      "|  Michael|  30|  NULL|         7|\n",
      "|   Sophia|  28| 55000|      NULL|\n",
      "|   Oliver|  35| 70000|        10|\n",
      "|      Ava|NULL| 45000|         2|\n",
      "|  William|  40|  NULL|        12|\n",
      "|Charlotte|  22| 40000|         1|\n",
      "|    David|  45| 80000|      NULL|\n",
      "+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any', thresh=3).show()\n",
    "\n",
    "# means not null should be 2 or less\n",
    "## Here not null value from each columns are 3 and more. hence non of them are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86f62cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+\n",
      "|     Name|Age|Salary|Experience|\n",
      "+---------+---+------+----------+\n",
      "|     John| 25| 50000|         5|\n",
      "|   Oliver| 35| 70000|        10|\n",
      "|Charlotte| 22| 40000|         1|\n",
      "+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any', thresh=4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dd2452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subset -- delete null value from column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7a91c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+----------+\n",
      "|     Name| Age|Salary|Experience|\n",
      "+---------+----+------+----------+\n",
      "|     John|  25| 50000|         5|\n",
      "|     Emma|NULL| 60000|         3|\n",
      "|   Sophia|  28| 55000|      NULL|\n",
      "|   Oliver|  35| 70000|        10|\n",
      "|      Ava|NULL| 45000|         2|\n",
      "|Charlotte|  22| 40000|         1|\n",
      "|    David|  45| 80000|      NULL|\n",
      "+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any', subset=['Salary']).show()\n",
    "\n",
    "# here null value from salary column is dropped and remain showing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d6987dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIlling the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c7b68f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+\n",
      "|     Name|Age|Salary|Experience|\n",
      "+---------+---+------+----------+\n",
      "|     John| 25| 50000|         5|\n",
      "|     Emma|  0| 60000|         3|\n",
      "|  Michael| 30|     0|         7|\n",
      "|   Sophia| 28| 55000|         0|\n",
      "|   Oliver| 35| 70000|        10|\n",
      "|      Ava|  0| 45000|         2|\n",
      "|  William| 40|     0|        12|\n",
      "|Charlotte| 22| 40000|         1|\n",
      "|    David| 45| 80000|         0|\n",
      "+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_pyspark.na.fill(\"missing values\").show() ---> it is not working because null values are in integer column\n",
    "\n",
    "df_pyspark.na.fill(0).show() # ---> It worked because we are replacing null of interger column with integer value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d2b6f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+----------+\n",
      "|     Name| Age|Salary|Experience|\n",
      "+---------+----+------+----------+\n",
      "|     John|  25| 50000|         5|\n",
      "|     Emma|NULL| 60000|         3|\n",
      "|  Michael|  30|     0|         7|\n",
      "|   Sophia|  28| 55000|      NULL|\n",
      "|   Oliver|  35| 70000|        10|\n",
      "|      Ava|NULL| 45000|         2|\n",
      "|  William|  40|     0|        12|\n",
      "|Charlotte|  22| 40000|         1|\n",
      "|    David|  45| 80000|      NULL|\n",
      "+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(0, 'Salary').show()  # For filling on specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "131aa5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+\n",
      "|     Name|Age|Salary|Experience|\n",
      "+---------+---+------+----------+\n",
      "|     John| 25| 50000|         5|\n",
      "|     Emma|  0| 60000|         3|\n",
      "|  Michael| 30|  NULL|         7|\n",
      "|   Sophia| 28| 55000|      NULL|\n",
      "|   Oliver| 35| 70000|        10|\n",
      "|      Ava|  0| 45000|         2|\n",
      "|  William| 40|  NULL|        12|\n",
      "|Charlotte| 22| 40000|         1|\n",
      "|    David| 45| 80000|      NULL|\n",
      "+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(0, ['Age']).show()  # --> We can pass the specific column whose null we want to fill, in list form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f0869ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+\n",
      "|     Name|Age|Salary|Experience|\n",
      "+---------+---+------+----------+\n",
      "|     John| 25| 50000|         5|\n",
      "|     Emma|  0| 60000|         3|\n",
      "|  Michael| 30|     0|         7|\n",
      "|   Sophia| 28| 55000|      NULL|\n",
      "|   Oliver| 35| 70000|        10|\n",
      "|      Ava|  0| 45000|         2|\n",
      "|  William| 40|     0|        12|\n",
      "|Charlotte| 22| 40000|         1|\n",
      "|    David| 45| 80000|      NULL|\n",
      "+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## For filling on multiple columns\n",
    "\n",
    "df_pyspark.na.fill(0, ['Age','Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dabc21f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+\n",
      "|     Name|Age|Salary|Experience|\n",
      "+---------+---+------+----------+\n",
      "|     John| 25| 50000|         5|\n",
      "|     Emma|  0| 60000|         3|\n",
      "|  Michael| 30|     0|         7|\n",
      "|   Sophia| 28| 55000|         0|\n",
      "|   Oliver| 35| 70000|        10|\n",
      "|      Ava|  0| 45000|         2|\n",
      "|  William| 40|     0|        12|\n",
      "|Charlotte| 22| 40000|         1|\n",
      "|    David| 45| 80000|         0|\n",
      "+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way\n",
    "\n",
    "df_pyspark.fillna({'Name': 'Missing values', 'Age': 0, 'Salary': 0, 'Experience': 0}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0a7e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+----------+\n",
      "|     Name| Age|Salary|Experience|\n",
      "+---------+----+------+----------+\n",
      "|     John|  25| 50000|         5|\n",
      "|     Emma|NULL| 60000|         3|\n",
      "|  Michael|  30|  NULL|         7|\n",
      "|   Sophia|  28| 55000|      NULL|\n",
      "|   Oliver|  35| 70000|        10|\n",
      "|      Ava|NULL| 45000|         2|\n",
      "|  William|  40|  NULL|        12|\n",
      "|Charlotte|  22| 40000|         1|\n",
      "|    David|  45| 80000|      NULL|\n",
      "+---------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7ba5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "# from pyspark.ml import Pipeline\n",
    "\n",
    "# define the columns to impute\n",
    "imputer_cols = ['Age', 'Salary', 'Experience']\n",
    "\n",
    "# create a Imputer object\n",
    "imputer = Imputer(\n",
    "    inputCols=imputer_cols, \n",
    "    outputCols=[col + '_imputed' for col in imputer_cols]\n",
    "    ).setStrategy(\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5cb46ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+----------+-----------+--------------+------------------+\n",
      "|     Name| Age|Salary|Experience|Age_imputed|Salary_imputed|Experience_imputed|\n",
      "+---------+----+------+----------+-----------+--------------+------------------+\n",
      "|     John|  25| 50000|         5|         25|         50000|                 5|\n",
      "|     Emma|NULL| 60000|         3|         32|         60000|                 3|\n",
      "|  Michael|  30|  NULL|         7|         30|         57142|                 7|\n",
      "|   Sophia|  28| 55000|      NULL|         28|         55000|                 5|\n",
      "|   Oliver|  35| 70000|        10|         35|         70000|                10|\n",
      "|      Ava|NULL| 45000|         2|         32|         45000|                 2|\n",
      "|  William|  40|  NULL|        12|         40|         57142|                12|\n",
      "|Charlotte|  22| 40000|         1|         22|         40000|                 1|\n",
      "|    David|  45| 80000|      NULL|         45|         80000|                 5|\n",
      "+---------+----+------+----------+-----------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit the pipeline to the data and transform the data\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()\n",
    "\n",
    "\n",
    "# All null values of given columns are filled with mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e180874",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For filling null with median\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "# from pyspark.ml import Pipeline\n",
    "\n",
    "# define the columns to impute\n",
    "imputer_cols = ['Age', 'Salary', 'Experience']\n",
    "\n",
    "# create a Imputer object\n",
    "imputer = Imputer(\n",
    "    inputCols=imputer_cols, \n",
    "    outputCols=[col + '_imputed' for col in imputer_cols]\n",
    "    ).setStrategy(\"median\")\n",
    "\n",
    "\n",
    "# Only mean is changed to median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a33a827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+----------+-----------+--------------+------------------+\n",
      "|     Name| Age|Salary|Experience|Age_imputed|Salary_imputed|Experience_imputed|\n",
      "+---------+----+------+----------+-----------+--------------+------------------+\n",
      "|     John|  25| 50000|         5|         25|         50000|                 5|\n",
      "|     Emma|NULL| 60000|         3|         30|         60000|                 3|\n",
      "|  Michael|  30|  NULL|         7|         30|         55000|                 7|\n",
      "|   Sophia|  28| 55000|      NULL|         28|         55000|                 5|\n",
      "|   Oliver|  35| 70000|        10|         35|         70000|                10|\n",
      "|      Ava|NULL| 45000|         2|         30|         45000|                 2|\n",
      "|  William|  40|  NULL|        12|         40|         55000|                12|\n",
      "|Charlotte|  22| 40000|         1|         22|         40000|                 1|\n",
      "|    David|  45| 80000|      NULL|         45|         80000|                 5|\n",
      "+---------+----+------+----------+-----------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit the pipeline to the data and transform the data\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()\n",
    "\n",
    "# All null values of given columns are filled with median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05df85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
